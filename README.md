
# MarkAI - AI Model Benchmarking Tool

**Tagline:**  
A comprehensive tool to evaluate and compare AI model performance across multiple metrics.

---

## 🚀 Project Overview

**MarkAI** is a lightweight, user-friendly, web-based benchmarking tool built to assess the performance of AI models. It offers detailed insights into model behavior including accuracy, inference time, and system resource usage, making it ideal for developers, IT professionals, and researchers.

---

## 👥 Team CodeStorm

- Reet Dubey  
- Swapna Swagatika Dhal  
- Varun Kushwah  
- Sarthak Maurya

---

## 🧠 Problem Statement

Build a lightweight web tool that allows users to upload AI models and receive quick performance insights such as:

- Accuracy  
- Inference time  
- Memory and resource usage

---

## 💡 Solution

MarkAI provides a modern benchmarking framework with both automated metrics analysis and an option for human-in-the-loop validation, reducing AI bias and improving result accuracy.

---

## 🛠️ Technical Details

**Architecture:**  
Client-server architecture

- **Client:** Web interface for uploading models, running benchmarks, and viewing results.  
- **Server:** Backend engine that processes model files and computes performance metrics.

**Tech Stack:**

- **Backend:** Python (Flask)  
- **Frontend:** HTML, CSS, JavaScript  
- **Database:** SQLite  
- **Deployment:** Netlify

---

## 🎮 How to Use

1. Access the web application through the live site.  
2. Sign up or log in.  
3. Upload a model and dataset.  
4. Select benchmark type.  
5. View real-time metrics and download performance reports.

---

## 🌍 Impact & Future Scope

MarkAI streamlines AI model evaluation, helping users identify performance bottlenecks and optimize their systems. Future improvements may include:

- GPU benchmarking  
- Support for more model formats  
- Auto-scaling deployment support  
- Integration with cloud storage

---

## 🙏 Acknowledgments

Special thanks to:

- Open-source communities of Flask and Netlify  
- Hackathon mentors for their guidance
